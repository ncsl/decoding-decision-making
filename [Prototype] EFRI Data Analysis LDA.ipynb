{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e42607c",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa5926f9",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0043744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import h5py \n",
    "import mat73\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from scipy.stats import ttest_1samp\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5fa72cf",
   "metadata": {},
   "source": [
    "## Load File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2010fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncsl_share = '/run/user/1000/gvfs/smb-share:server=10.162.37.21,share=main'\n",
    "# data_path = f'Data/Subject06_snapshot_normalized.npy'\n",
    "subs = ['06']\n",
    "file_paths = {}\n",
    "\n",
    "for sub in subs:\n",
    "    # create a dictionary holding the file paths\n",
    "    ncsl_share = '/mnt/ncsl_share'\n",
    "    file_paths[sub] = {\n",
    "        'setup_path': ncsl_share + f'/Public/EFRI/1_formatted/SUBJECT{sub}/EFRI{sub}_WAR_SES1_Setup.mat',\n",
    "        'raw_path': ncsl_share + f'/Public/EFRI/1_formatted/SUBJECT{sub}/EFRI{sub}_WAR_SES1_Raw.mat',\n",
    "        'data_path': ncsl_share + f'/Daniel/Data/Trial_by_Chan_by_Freq_by_Time_Snapshots/Subject{sub}_snapshot_normalized.npy', # movement onset as event\n",
    "        # 'data_path' : ncsl_share + f'/Daniel/Data/Trial_by_Chan_by_Freq_by_Time_Snapshots/show-card_pre-2sec_post-4sec/Subject{sub}_snapshot_normalized.npy', # visual cue as event\n",
    "        'out_path_metrics': f'Metrics/Subject{sub}',\n",
    "        'out_path_plots': f'Plots/Subject{sub}'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f5fd36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file = h5py.File(file_paths['06']['raw_path'])\n",
    "setup_data = mat73.loadmat(file_paths['06']['setup_path'])\n",
    "\n",
    "out_path_plots = file_paths['06']['out_path_plots']\n",
    "out_path_metrics = file_paths['06']['out_path_metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f36eceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['elec_area', 'elec_ind', 'elec_name', 'filters', 'trial_times', 'trial_words'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_data.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43ba6ff0",
   "metadata": {},
   "source": [
    "## Instantiate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "530612d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bets = setup_data['filters']['bets']\n",
    "\n",
    "good_trials = np.where(np.isnan(bets) == False)[0] # extract indices of trials without the 'nan'\n",
    "\n",
    "bets = bets[good_trials] # get the bet values for the good trials\n",
    "subject_cards = setup_data['filters']['card1'][good_trials] # get the subject's card values for the good trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd25babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_names = np.array(setup_data['elec_name'])\n",
    "elec_areas = np.array(setup_data['elec_area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed8d5dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(file_paths['06']['data_path'])\n",
    "y = np.asarray([(0 if bet == 5 else 1) for bet in bets]) # 0 = low bet ($5), 1 = high bet ($20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f423537",
   "metadata": {},
   "source": [
    "## Matplotlib Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "737cec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.titlesize'] = 22\n",
    "mpl.rcParams['axes.labelsize'] = 18\n",
    "mpl.rcParams['xtick.labelsize'] = 18\n",
    "mpl.rcParams['ytick.labelsize'] = 18"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca8b194d",
   "metadata": {},
   "source": [
    "## Create Frequency Bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54340c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelet_freqs = np.logspace(np.log2(2),np.log2(150),num=63,base=2)\n",
    "\n",
    "frequency_band_indices ={\n",
    "    \"Delta\" : [i for i,freq in enumerate(wavelet_freqs) if freq >= 0.5 and freq < 4],\n",
    "    \"Theta\" : [i for i,freq in enumerate(wavelet_freqs) if freq >= 4 and freq < 8],\n",
    "    \"Alpha\" : [i for i,freq in enumerate(wavelet_freqs) if freq >= 8 and freq < 14],\n",
    "    \"Beta\" : [i for i,freq in enumerate(wavelet_freqs) if freq >= 14 and freq < 30],\n",
    "    \"Gamma\" : [i for i,freq in enumerate(wavelet_freqs) if freq >= 30]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bc2f496",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_band_data = np.zeros((data.shape[0], data.shape[1], 5, data.shape[3]))\n",
    "\n",
    "for i, key in enumerate(frequency_band_indices):\n",
    "    f_band_data[:,:,i,:] = data[:,:,frequency_band_indices[key],:].mean(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b764b5b5",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0715a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_sampled_channels(channels, sample_size, sampled_channels_=[]):\n",
    "    \"\"\"Recursively generate a list (size = sample_size) of lists of sampled channels\"\"\"\n",
    "    np.random.seed()\n",
    "\n",
    "    if len(channels) > sample_size:\n",
    "        sample = np.random.choice(channels, size=sample_size, replace=False)\n",
    "        sampled_channels_.append(list(sample))\n",
    "        channels = np.delete(channels, np.where(np.isin(channels, sample))[0])\n",
    "        _generate_sampled_channels(channels, sample_size, sampled_channels_)\n",
    "    else:\n",
    "        sampled_channels_.append(list(channels))\n",
    "\n",
    "    return sampled_channels_\n",
    "\n",
    "def _find_combinations(n, k):\n",
    "    \"\"\"Find all possible combinations of k channels from n channels\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    combinations_list : list\n",
    "        List of tuples containing the indices all possible combinations of k channels from n channels\n",
    "    \"\"\"\n",
    "    population = list(range(0, n))\n",
    "    combinations_list = list(itertools.combinations(population, k))\n",
    "    return combinations_list\n",
    "\n",
    "def _get_collective_predictions(predictions):\n",
    "    # Get the collective prediction for each trial\n",
    "    collective_predictions = []\n",
    "\n",
    "    for trial_predictions in predictions:\n",
    "        if trial_predictions.mean() >= 0.5:\n",
    "            collective_predictions.append(1)\n",
    "        else:\n",
    "            collective_predictions.append(0)\n",
    "    \n",
    "    return collective_predictions\n",
    "\n",
    "def _get_collective_prediction_accuracy(collective_predictions, y):\n",
    "    # Get the accuracy of theveloped a circuit model of decision-making which accounts for the specificity of inputs to and outputs from inhibitory neurons. We found that selective inhibition expands the space of circuits supporting decision-making, allowing for weaker or stronger recurrent excitation when connected in a competitive or feedback motif. The specificity of inhibitory outputs sets te collective prediction\n",
    "    accuracy = (y == collective_predictions).mean()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da9e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator(object):\n",
    "    def __init__(self, data, setup_data):\n",
    "        self._num_trials, self._num_channels, self._num_freqs, self._num_timesteps = data.shape\n",
    "        \n",
    "        self._elec_areas = setup_data['elec_area']\n",
    "        self._elec_names = setup_data['elec_name']\n",
    "\n",
    "    def create_X(self, data, channel, time):\n",
    "        \"\"\"Create the X data that will be used to train the LDA model\"\"\"\n",
    "        if hasattr(self, '_time_resolution') and type(self._time_resolution) == int:\n",
    "            X = data[:, channel, :, time:time+self._time_resolution].mean(-1)\n",
    "        elif type(time) == list and len(time) == 2:\n",
    "            if time[0] - time[1] == 0:\n",
    "                X = data[:, channel, :, time[0]]\n",
    "            else:\n",
    "                X = data[:, channel, :, time[0]:time[1]].mean(-1)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def filter_channels(self):\n",
    "        \"\"\"Filters out channels that are in particular anatomical locations\"\"\"\n",
    "        filtered_elec_areas_idxs = [i for i,ea in enumerate(self._elec_areas) if ea not in ['white matter','CZ','PZ', 'out','FZ','cerebrospinal fluid','lesion L','ventricle L','ventricle R']]\n",
    "        filtered_elec_areas = [self._elec_areas[i] for i in filtered_elec_areas_idxs]\n",
    "        filtered_num_channels = len(filtered_elec_areas_idxs)\n",
    "\n",
    "        return filtered_elec_areas_idxs, filtered_elec_areas, filtered_num_channels\n",
    "    \n",
    "    def set_attributes(self, **kwargs):\n",
    "        \"\"\"Set class attributes specified by the dataset and metadata\"\"\"\n",
    "        if 'time_resolution' in kwargs:\n",
    "            if not(self._num_timesteps % kwargs['time_resolution'] == 0):\n",
    "                raise Exception(\"Invalid time resolution size, num_timesteps % resolution must equal 0\")\n",
    "            else:\n",
    "                self._time_resolution = kwargs['time_resolution']\n",
    "                self._timesteps_rescaled = int(self._num_timesteps/kwargs['time_resolution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81564d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA(Estimator):\n",
    "    def __init__(self, data, setup_data):\n",
    "        super().__init__(data, setup_data)\n",
    "        self._reset_metrics()\n",
    "        \n",
    "    def _reshape_attributes(self, new_shape:tuple):\n",
    "        \"\"\"Reshape class attributes to specified shape\"\"\"\n",
    "        for attr_name in self.__dict__.keys():\n",
    "            if not attr_name.startswith('_'):\n",
    "                setattr(self, attr_name, np.reshape(getattr(self, attr_name), new_shape))\n",
    "\n",
    "    def _reset_metrics(self):\n",
    "        self.mean_scores = []\n",
    "        self.std_scores = []\n",
    "        self.dvals = []\n",
    "        self.t_stats = []\n",
    "        self.p_vals = []\n",
    "        self.low_bet_avg_powers = []\n",
    "        self.high_bet_avg_powers = []\n",
    "        self.diff_avg_powers = []\n",
    "        self.lda_coefs = []\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"Train LDA model on specified X data and y labels\"\"\"\n",
    "        low_bet_avg_powers = X[np.where(y == 0)].mean(0)\n",
    "        high_bet_avg_powers = X[np.where(y == 1)].mean(0)\n",
    "        diff_avg_powers = high_bet_avg_powers - low_bet_avg_powers\n",
    "\n",
    "        self.high_bet_avg_powers.append(high_bet_avg_powers)\n",
    "        self.low_bet_avg_powers.append(low_bet_avg_powers)\n",
    "        self.diff_avg_powers.append(diff_avg_powers)\n",
    "\n",
    "        # Using RepeatedKFold() for training LDA\n",
    "        rkf = RepeatedKFold(n_splits=5, n_repeats=1, random_state=42)\n",
    "\n",
    "        estimators = []\n",
    "        scores = []\n",
    "        dval = np.zeros(self._num_trials)\n",
    "\n",
    "        for train, test in rkf.split(X):\n",
    "            lda = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "            lda.fit(X[train], y[train])\n",
    "            estimators.append(lda)\n",
    "            scores.append(lda.score(X[test], y[test]))\n",
    "            dval[test] = np.dot(X[test], lda.coef_.T).T[0] + lda.intercept_\n",
    "\n",
    "        self.dvals.append(dval)\n",
    "        self.lda_coefs.append(estimators[scores.index(max(scores))].coef_[0])\n",
    "        self.mean_scores.append(np.mean(scores))\n",
    "        \n",
    "        self.std_scores.append(np.std(scores))\n",
    "\n",
    "        t_stat, p_val = ttest_1samp(dval, popmean=0) # perform 1-sided t-test on decision values corresponding to high bet\n",
    "        \n",
    "        self.t_stats.append(t_stat)\n",
    "        self.p_vals.append(p_val)\n",
    "    \n",
    "    def create_cluster_idxs(self, threshold):\n",
    "        \"\"\"Creates statistical clusters based on the t-statistics computed from LDA decision values\"\"\"\n",
    "        if self.t_stats.shape == (self._num_channels, self._timesteps_rescaled, 1):\n",
    "            cluster_idxs = []\n",
    "\n",
    "            for channel in range(self._num_channels):\n",
    "                ch_cluster_idxs = []\n",
    "                threshold_idxs = [i for i, t_stat in enumerate(self.t_stats[channel].flatten()) if np.abs(t_stat) > threshold]\n",
    "                temp_cluster_idxs = [threshold_idxs[0]]\n",
    "            \n",
    "                # Groups consecutive clusters together\n",
    "                for i in range(len(threshold_idxs) - 1):\n",
    "                    if threshold_idxs[i+1] - threshold_idxs[i] == 1:\n",
    "                        temp_cluster_idxs.append(threshold_idxs[i+1])\n",
    "                    else:\n",
    "                        ch_cluster_idxs.append(temp_cluster_idxs)\n",
    "                        temp_cluster_idxs = [threshold_idxs[i+1]]\n",
    "                \n",
    "                if len(temp_cluster_idxs) != 0:\n",
    "                    ch_cluster_idxs.append(temp_cluster_idxs)\n",
    "                \n",
    "                cluster_idxs.append(ch_cluster_idxs)\n",
    "            \n",
    "            return cluster_idxs\n",
    "        else:\n",
    "            Exception('Cannot create clusters with these attributes, make sure shape of attributes is (num_channels, timesteps_rescaled, 1)')\n",
    "\n",
    "    def compute_t_stat_clusters(self, threshold):\n",
    "        \"\"\"Computes the sum of the t-statistics in each cluster\"\"\"\n",
    "        t_stat_sums = []\n",
    "\n",
    "        for channel in range(self._num_channels):\n",
    "            temp_t_stat_sums = []\n",
    "            for arr in self.create_cluster_idxs(threshold)[channel]:\n",
    "                temp_t_stat_sums.append(self.t_stats[channel][arr].sum())\n",
    "            \n",
    "            t_stat_sums.append(temp_t_stat_sums)\n",
    "        \n",
    "        return t_stat_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3ac6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainOptimalTimeWindows(LDA):\n",
    "    def __init__(self, data, setup_data) -> None:\n",
    "        super().__init__(data, setup_data)\n",
    "\n",
    "    def _create_time_windows(self):\n",
    "        \"\"\"Create all possible time windows from which X data can be created.\"\"\"\n",
    "        time_windows = []\n",
    "        for i in range(self._num_timesteps):\n",
    "            for j in range(self._num_timesteps):\n",
    "                if i-j >= 0 and i+j <= self._num_timesteps:\n",
    "                    time_windows.append([i-j,i+j])\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "        return time_windows\n",
    "    \n",
    "    def _get_predictions(self):\n",
    "        predictions = []\n",
    "\n",
    "        # Get predictions for each trial by each channel\n",
    "        for trial in range(self._num_trials):\n",
    "            trial_predictions = []\n",
    "            for dval in self.dvals[:,trial]:\n",
    "                if dval >= 0:\n",
    "                    channel_prediction = 1\n",
    "                else:\n",
    "                    channel_prediction = 0\n",
    "                trial_predictions.append(channel_prediction)\n",
    "            \n",
    "            predictions.append(trial_predictions)\n",
    "\n",
    "        predictions = np.asarray(predictions)\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    def _grid_search_on_channel_combinations(self, y, max_channels=20):\n",
    "        assert max_channels <= 20, 'Cannot perform grid search on more than 20 channels'\n",
    "\n",
    "        predictions = self._get_predictions()\n",
    "            # Find all possible channel combinations to use for collective prediction\n",
    "        all_channel_idxs_combinations = []\n",
    "        for i in range(max_channels):\n",
    "            # Find all possible channel combinations of length i+1\n",
    "            channel_combinations = _find_combinations(max_channels, i+1)\n",
    "            all_channel_idxs_combinations.append(channel_combinations)\n",
    "\n",
    "        # Grid search on optimal channel combination to use for collective prediction\n",
    "        accuracies = []\n",
    "\n",
    "        for k_length_combinations in all_channel_idxs_combinations:\n",
    "            k_length_combination_accuracies = []\n",
    "            for combination in k_length_combinations:\n",
    "                # Stores all the predictions for this particular combination of k channels\n",
    "                combination = list(combination)\n",
    "                combination_predictions = predictions[:,combination]\n",
    "\n",
    "                # Stores the collective prediction for this particular combination of k channels\n",
    "                collective_combination_predictions = _get_collective_predictions(combination_predictions)\n",
    "\n",
    "                # Get the accuracy of the collective prediction for each combination of k channels\n",
    "                k_length_combination_accuracies.append(_get_collective_prediction_accuracy(collective_combination_predictions, y))\n",
    "            \n",
    "            accuracies.append(k_length_combination_accuracies)\n",
    "\n",
    "        return all_channel_idxs_combinations, accuracies\n",
    "\n",
    "    def _time_window_grid_search(self, data, y, channels):\n",
    "        \"\"\"Train LDA model on all possible time windows, store the time windows that correspond with the highest LDA score.\"\"\"\n",
    "\n",
    "        time_windows = self._create_time_windows()\n",
    "        best_time_windows = []\n",
    "\n",
    "        for channel in channels:\n",
    "            for times in time_windows:\n",
    "                X = super().create_X(data, channel, times)\n",
    "                super().train(X,y)\n",
    "\n",
    "            print(f'Channel {channel} done')\n",
    "            best_time_windows.append([channel, time_windows[np.argmax(self.mean_scores)], np.max(self.mean_scores)])\n",
    "            super()._reset_metrics()\n",
    "        \n",
    "        return best_time_windows\n",
    "\n",
    "    def _multiprocessing_time_window_grid_search(self, data, y, n_processes, filter_channels:bool = True):\n",
    "        \"\"\"Perform a time window grid search in parallel\"\"\"\n",
    "        if filter_channels:\n",
    "            filtered_elec_areas_idxs, filtered_elec_areas, filtered_num_channels = super().filter_channels()\n",
    "            channels = filtered_elec_areas_idxs\n",
    "        else:\n",
    "            channels = np.arange(self._num_channels)\n",
    "\n",
    "        sample_size = round(len(channels)/n_processes)\n",
    "\n",
    "        sampled_channels = _generate_sampled_channels(channels, sample_size, [])\n",
    "\n",
    "        # if __name__ == '__main__':\n",
    "        with Pool(n_processes) as p:\n",
    "            results = p.starmap(self._time_window_grid_search, [(data, y, channels) for channels in sampled_channels])\n",
    "            p.close()\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def train_on_optimal_time_windows(self, data, y, n_processes, n_channels=10, filter_channels:bool=True):\n",
    "        \"\"\"Train LDA model on the optimal time windows for top performing channels, specified by n_channels\"\"\"\n",
    "        results = self._multiprocessing_time_window_grid_search(data, y, n_processes, filter_channels=filter_channels)\n",
    "\n",
    "        # Unravel the results from the multiprocessing and sort them by channels\n",
    "        optimal_time_windows_per_channel = [item for sublist in results for item in sublist]\n",
    "        optimal_time_windows_per_channel.sort(key=lambda x: x[0])\n",
    "        optimal_time_windows_per_channel.sort(key=lambda x: x[2], reverse=True)\n",
    "        self._optimal_time_windows_per_channel = optimal_time_windows_per_channel\n",
    "\n",
    "        for channel, times, _ in optimal_time_windows_per_channel[:n_channels]:\n",
    "            X = super().create_X(data, channel, times)\n",
    "            super().train(X,y)\n",
    "        \n",
    "        super()._reshape_attributes((n_channels,-1))\n",
    "\n",
    "    def get_optimal_channel_combination(self, y, max_channels=20):\n",
    "        \"\"\"\n",
    "        Get the optimal channel combination to use for collective prediction. \n",
    "        Channels used to find combination is specified by max_channels.\"\"\"\n",
    "\n",
    "        all_channel_idxs_combinations, accuracies = self._grid_search_on_channel_combinations(y, max_channels=max_channels)\n",
    "\n",
    "        optimal_time_windows_per_ch = self._optimal_time_windows_per_channel\n",
    "        max_accuracies = []\n",
    "\n",
    "        for i, accuracies in enumerate(accuracies):\n",
    "            optimal_chs = []\n",
    "            ch_idxs = all_channel_idxs_combinations[i][np.argmax(accuracies)]\n",
    "            for idx in ch_idxs:\n",
    "                optimal_chs.append(optimal_time_windows_per_ch[idx][0])\n",
    "\n",
    "            max_accuracies.append([optimal_chs, np.max(accuracies)])\n",
    "\n",
    "        max_accuracies.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return max_accuracies\n",
    "    \n",
    "    def plot_accuracies(self, y):\n",
    "        predictions = self._get_predictions()\n",
    "        accuracies = []\n",
    "\n",
    "        for i in range(predictions.shape[1]):\n",
    "            # Get collective prediction of channels 0 to i\n",
    "            collective_predictions = _get_collective_predictions(predictions[:,:i+1])\n",
    "            accuracies.append(_get_collective_prediction_accuracy(collective_predictions, y))\n",
    "        \n",
    "        peak_accuracy_group_idx = np.argmax(accuracies)\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "        ax.plot(accuracies)\n",
    "        ax.set_title('Accuracy of Majority Concensus')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_xlabel('Number of Top Performing Channels in Group')\n",
    "        ax.axvline(peak_accuracy_group_idx, color = 'red', alpha=0.5)\n",
    "        ax.annotate(f'(Group Size for Peak Aaccuracy: {peak_accuracy_group_idx + 1} Score: {accuracies[peak_accuracy_group_idx]})', xy=(peak_accuracy_group_idx,np.mean(accuracies)), fontsize = 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e0c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerChannelTimestep(LDA):\n",
    "    \"\"\"Visualizes model performance for LDA models trained on each channel and timestep of the data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        setup_data\n",
    "        ) :\n",
    "        super().__init__(data, setup_data)\n",
    "\n",
    "    def _sort_scores(self, filter_channels:bool):\n",
    "        \"\"\"Sort channels from greatest to least maximum LDA scores, and indicate timepoint at which maximum LDA score occurs.\"\"\"\n",
    "        max_mean_scores = np.zeros((self._num_channels,3))\n",
    "\n",
    "        for channel in range(self._num_channels):\n",
    "            max_mean_scores[channel, 0] = int(channel) # store the channel index\n",
    "            max_mean_scores[channel, 1] = list(self.mean_scores[channel]).index(max(self.mean_scores[channel])) # the time point at which the maximum mean score occurs\n",
    "            max_mean_scores[channel, 2] = max(self.mean_scores[channel]) # value of the maximum mean score in a particular channel for all time points\n",
    "\n",
    "        if filter_channels == True:\n",
    "            filtered_elec_areas_idxs, _, __ = super().filter_channels()\n",
    "            sorted_indices = max_mean_scores[filtered_elec_areas_idxs,2].argsort()[::-1]\n",
    "            self.sorted_max_mean_scores = max_mean_scores[filtered_elec_areas_idxs][sorted_indices]\n",
    "            self.sorted_elec_names = [self._elec_names[i] for i in np.int_(self.sorted_max_mean_scores[:, 0])]\n",
    "            self.sorted_elec_areas = [self._elec_areas[i] for i in np.int_(self.sorted_max_mean_scores[:, 0])]\n",
    "            \n",
    "        else:\n",
    "            sorted_indices = max_mean_scores[:,2].argsort()[::-1]\n",
    "            self.sorted_max_mean_scores = max_mean_scores[sorted_indices]\n",
    "            self.sorted_elec_names = [self._elec_names[i] for i in sorted_indices]\n",
    "            self.sorted_elec_areas = [self._elec_areas[i] for i in sorted_indices]\n",
    "\n",
    "    def train_per_channel_and_timestep(self, data, y, time_resolution, filter_channels:bool = True):\n",
    "        \"\"\"Train an LDA model on each channel and timestep\"\"\"\n",
    "        super().set_attributes(time_resolution=time_resolution)\n",
    "\n",
    "        for channel in range(self._num_channels):\n",
    "            for time in range(self._timesteps_rescaled):\n",
    "                X = super().create_X(data, channel, time*self._time_resolution)\n",
    "                super().train(X, y)\n",
    "\n",
    "        super()._reshape_attributes((self._num_channels,self._timesteps_rescaled,-1))\n",
    "        self._sort_scores(filter_channels)\n",
    "\n",
    "    def plot_sorted_scores(self, out_path:str):\n",
    "        \"\"\"Visualize the LDA model scores (sorted from greatest to least) for all channels.\"\"\"\n",
    "        num_channels = len(self.sorted_max_mean_scores)\n",
    "        \n",
    "        fig, axs = plt.subplots(3, 1, figsize=(24,24), gridspec_kw={'height_ratios' : [1,1,1]})\n",
    "\n",
    "        axs[0].set_title('Sorted Peak Score of LDA Models (from greatest to least)')\n",
    "        axs[0].set_ylabel('Peak Mean Score')\n",
    "        axs[0].set_xlabel('Channels (from most to least accurate)')\n",
    "        axs[0].plot(np.arange(0,num_channels,1), self.sorted_max_mean_scores[:,2])\n",
    "\n",
    "        axs[1].set_title('Scatter Plot of Peak Score of LDA Models (from greatest to least) with error bars')\n",
    "        axs[1].set_ylabel('Peak Mean Score')\n",
    "        axs[1].set_xlabel('Channels (from most to least accurate)')\n",
    "        axs[1].scatter(np.arange(0,num_channels,1), self.sorted_max_mean_scores[:,2])\n",
    "        axs[1].errorbar(np.arange(0,num_channels,1), self.sorted_max_mean_scores[:,2], yerr=self.std_scores[np.int_(self.sorted_max_mean_scores[:,0]),np.int_(self.sorted_max_mean_scores[:,1])].flatten(), fmt=\"o\")\n",
    "        \n",
    "        axs[2].set_title('Time of Peak Score of LDA Models')\n",
    "        axs[2].set_ylabel('Time (seconds)')\n",
    "        axs[2].set_xlabel('Channels (from most to least accurate)')\n",
    "        time = self.sorted_max_mean_scores[:,1]/(20/self._time_resolution) - 3 # LOOKUP\n",
    "        axs[2].scatter(np.arange(0, num_channels), time)\n",
    "        axs[2].axhline(y = 0, color = 'red', alpha=0.5)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(out_path + f'_sorted_scores')\n",
    "        \n",
    "        plt.figure(figsize=(24,24))\n",
    "        plt.title('Sorted Peak Score of LDA Models (from greatest to least)')\n",
    "        plt.ylabel('Channel Names')\n",
    "        plt.xlabel('Score')\n",
    "        plt.hlines(y=np.arange(0,num_channels), xmin=0.5, xmax=self.sorted_max_mean_scores[:,2][::-1],\n",
    "            color='blue', alpha=0.6, linewidth=2)\n",
    "        plt.yticks(np.arange(0,num_channels), labels=self.sorted_elec_areas[::-1])\n",
    "\n",
    "        plt.savefig(out_path + f'_sorted_scores_hline')\n",
    "    \n",
    "    def plot_sorted_scores_per_channel(self, num_plots:int, out_path:str):\n",
    "        \"\"\"Visualize the LDA model scores over time for top performing channels.\"\"\"\n",
    "        time_resolution = self._time_resolution\n",
    "        timesteps_rescaled = self._timesteps_rescaled\n",
    "        \n",
    "        times = (np.arange(0, timesteps_rescaled, 1) / (20/time_resolution)) - 3 # time 0 seconds denotes when the subject starts moving (i.e. 3 seconds into the data)\n",
    "\n",
    "        fig, axs = plt.subplots(num_plots, 1, figsize=(24, 8 * num_plots))\n",
    "\n",
    "        for i, trial_data in enumerate(self.sorted_max_mean_scores[:num_plots]):\n",
    "            channel, time, peak_accuracy = trial_data\n",
    "            time = time/(20/time_resolution) - 3\n",
    "            ax = axs[i]\n",
    "            ax.plot(times[:], self.mean_scores[int(channel)])\n",
    "            ax.set_title('Electrode %s in the %s' %(self.sorted_elec_names[i], self.sorted_elec_areas[i]))\n",
    "            ax.set_ylabel('Score')\n",
    "            ax.set_xlabel('Time (seconds)')\n",
    "            # ax.tick_params(axis='both', labelsize=12)\n",
    "            ax.axvline(time, color = 'red', alpha=0.5)\n",
    "            ax.axvline(0, color = 'blue', alpha=0.5, ls = '--')\n",
    "            ax.annotate(f'(Time: {time:.2f}s, Score: {peak_accuracy:.2f})', xy=(time + .05 ,.6), fontsize = 12)\n",
    "        \n",
    "        plt.savefig(out_path + f'_sorted_scores_per_channel')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_power_heatmap(self, plot_metric, num_plots:int, out_path:str):\n",
    "        \"\"\"Plot heatmaps of high/low frequency powers, difference in high/low frequency powers, and LDA coefficients.\"\"\"\n",
    "        num_freqs = self._num_freqs\n",
    "        time_resolution = self._time_resolution\n",
    "        timesteps_rescaled = self._timesteps_rescaled\n",
    "\n",
    "        if timesteps_rescaled >= 50:\n",
    "            num_xticks = int(timesteps_rescaled/2)\n",
    "        else:\n",
    "            num_xticks = timesteps_rescaled\n",
    "\n",
    "        xticks = np.linspace(0, timesteps_rescaled - 1, num=num_xticks, dtype=np.int_)\n",
    "        xticklabels = np.around(np.linspace(0, timesteps_rescaled - 1, num=num_xticks, dtype=np.int_)/(20/time_resolution) - 3, decimals=2)\n",
    "\n",
    "        if num_freqs == 5: \n",
    "            # sets the y-tick labels for EEG frequency bands\n",
    "            yticks = np.arange(num_freqs)\n",
    "            yticklabels = [\"Delta\", \"Theta\", \"Alpha\", \"Beta\", \"Gamma\"]\n",
    "        else:\n",
    "            # sets the y-tick labels for all frequencies in data\n",
    "            yticks = np.arange(num_freqs, step=2)\n",
    "            yticklabels = [round(i,1) for i in np.logspace(np.log2(2),np.log2(150), num = len(yticks),base=2, dtype=np.float16)]\n",
    "\n",
    "        for i, trial_data in enumerate(self.sorted_max_mean_scores[:num_plots]):\n",
    "            channel, time, peak_accuracy = trial_data\n",
    "\n",
    "            low_bet_powers = self.low_bet_avg_powers[int(channel)]\n",
    "            high_bet_powers = self.high_bet_avg_powers[int(channel)]\n",
    "            diff_bet_powers = self.diff_avg_powers[int(channel)]\n",
    "            lda_coef = self.lda_coefs[int(channel)]\n",
    "\n",
    "            fig, axs = plt.subplots(2, 2,figsize=(24, 20))\n",
    "            \n",
    "            # Plot power per frequency as a function of time, power averaged across all respective trials (high or low bet trials) \n",
    "            sns.heatmap(high_bet_powers.T, ax=axs[0][0], vmin=-.4, vmax=.4, cbar_kws={\"label\": \"Z-Scored Frequency Power\"}, cmap='PRGn')\n",
    "            axs[0][0].set_title('Electrode %s in the %s \\n High Bet Z-Scored Frequency Power (n = %s)' %(self.sorted_elec_names[i], self.sorted_elec_areas[i], \"~\"))\n",
    "            axs[0][0].set(xlabel=\"Time (sec)\", ylabel=\"Frequency (Hz)\")\n",
    "\n",
    "            sns.heatmap(low_bet_powers.T, ax=axs[0][1], vmin=-.4, vmax=.4, cbar_kws={\"label\": \"Z-Scored Frequency Power\"}, cmap='PRGn')\n",
    "            axs[0][1].set_title('Electrode %s in the %s \\n Low Bet Z-Scored Frequency Power (n = %s)' %(self.sorted_elec_names[i], self.sorted_elec_areas[i], \"~\"))\n",
    "            axs[0][1].set(xlabel=\"Time (sec)\", ylabel=\"Frequency (Hz)\")\n",
    "\n",
    "            # Plots the difference in power frequency for high and low bet trials\n",
    "            sns.heatmap(diff_bet_powers.T, ax=axs[1][0], vmin=-.4, vmax=.4, cbar_kws={\"label\": \"Z-Scored Frequency Power\", \"pad\": 0.1}, cmap='PRGn')\n",
    "            axs[1][0].set_title('Electrode %s in the %s \\n Difference in Z-Scored Frequency Power (High - Low Bet)' %(self.sorted_elec_names[i], self.sorted_elec_areas[i]))\n",
    "            axs[1][0].set(xlabel=\"Time (sec)\", ylabel=\"Frequency (Hz)\")\n",
    "            ax = axs[1][0].twinx()\n",
    "            sns.lineplot(x=np.arange(0,timesteps_rescaled), y=plot_metric[int(channel)].flatten(), color='blue', ax=ax) # Make the overlayed metric an optional variable user can select\n",
    "            ax.set_ylabel('Mean LDA Score')\n",
    "            axs[1][0].axvline(time, color = 'red', alpha=0.5)\n",
    "            axs[1][0].axvline(0, color = 'blue', alpha=0.25, ls = '--')\n",
    "\n",
    "            # Plots the LDA coefficients for each frequency band over time\n",
    "            sns.heatmap(lda_coef.T, ax=axs[1][1], vmin=-1, vmax=1, cbar_kws={\"label\": \"Z-Scored Frequency Power\"}, cmap='PRGn')\n",
    "            axs[1][1].set_title('LDA coefficient values for all frequencies \\n at %s in %s' %(self.sorted_elec_names[i], self.sorted_elec_areas[i]))\n",
    "            axs[1][1].set(xlabel=\"Time (sec)\", ylabel=\"Frequency (Hz)\")\n",
    "\n",
    "            for axs_ in axs:\n",
    "                for ax in axs_:\n",
    "                    ax.set_xticks(xticks, labels = xticklabels)\n",
    "                    ax.set_yticks(yticks)\n",
    "                    ax.set_yticklabels(yticklabels, rotation = 0)\n",
    "                    ax.axes.invert_yaxis()\n",
    "                    ax.axvline(time, color = 'red', alpha=0.5)\n",
    "                    ax.axvline(12, color = 'blue', alpha=0.5, ls = '--')\n",
    "\n",
    "                    for label in ax.xaxis.get_ticklabels()[1::2]:\n",
    "                        label.set_visible(False)\n",
    "\n",
    "            plt.savefig(out_path + '_heatmap_%s_%s'%(self.sorted_elec_names[i], self.sorted_elec_areas[i]))\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28fc765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerTimestepAllChannels(LDA):\n",
    "\n",
    "    \"\"\"Visualizes model performance for LDA models trained on all channels at each timestep of the data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        setup_data\n",
    "        ) :\n",
    "        super().__init__(data, setup_data)\n",
    "        \n",
    "    def _sort_scores(self):\n",
    "        \"\"\"Sort the LDA scores from greatest to least, with indexes of channels saved.\"\"\"\n",
    "        enumerated_mean_scores = np.array(list(enumerate(self.mean_scores.flatten())))\n",
    "        sorted_indices = enumerated_mean_scores[:,1].argsort()[::-1]\n",
    "        self.sorted_mean_scores = enumerated_mean_scores[sorted_indices]\n",
    "\n",
    "    def _convert_timesteps_to_time(self, event_delay):\n",
    "        \"\"\"Convert timesteps into seconds while specifying timepoint 0.\"\"\"\n",
    "        self.__event_delay = event_delay\n",
    "        # time point \"0 seconds\" denoted by event_delay\n",
    "        self.__times = (np.arange(0, self._timesteps_rescaled) / (20/self._time_resolution)) - event_delay \n",
    "\n",
    "    \n",
    "    def plot_sorted_scores(self, out_path:str):\n",
    "        \"\"\"Visualize the LDA model scores (sorted from greatest to least) for all timepoints.\"\"\"\n",
    "        num_timesteps = self._timesteps_rescaled\n",
    "        xticks = np.arange(0,num_timesteps,1)\n",
    "        \n",
    "        fig, axs = plt.subplots(3, 1, figsize=(24,24), gridspec_kw={'height_ratios' : [1,1,1]})\n",
    "\n",
    "        axs[0].set_title('Sorted Peak Score of LDA Models (from greatest to least)')\n",
    "        axs[0].set_ylabel('Mean Score')\n",
    "        axs[0].set_xlabel('Times (from most to least accurate)')\n",
    "        axs[0].set_xticks(xticks, labels = self.__times[np.int_(self.sorted_mean_scores[:,0])])\n",
    "        axs[0].plot(xticks ,self.sorted_mean_scores[:,1])\n",
    "\n",
    "        axs[1].set_title('Scatter Plot of Peak Score of LDA Models (from greatest to least) with error bars')\n",
    "        axs[1].set_ylabel('Mean Score')\n",
    "        axs[1].set_xlabel('Times (from most to least accurate)')\n",
    "        axs[1].set_xticks(xticks, labels = self.__times[np.int_(self.sorted_mean_scores[:,0])])\n",
    "        axs[1].scatter(xticks, self.sorted_mean_scores[:,1])\n",
    "        axs[1].errorbar(xticks, self.sorted_mean_scores[:,1], yerr=self.std_scores[np.int_(self.sorted_mean_scores[:,0])].flatten(), fmt=\"o\")\n",
    "\n",
    "        axs[2].set_title('Sorted Peak Score of LDA Models (from greatest to least)')\n",
    "        axs[2].set_ylabel('Times (from most to least accurate)')\n",
    "        axs[2].set_xlabel('Score')\n",
    "        axs[2].hlines(y=np.arange(0,num_timesteps), xmin=0.5, xmax=self.sorted_mean_scores[:,1][::-1],\n",
    "            color='blue', alpha=0.6, linewidth=2)\n",
    "        axs[2].set_yticks(np.arange(0,num_timesteps), labels=self.__times[np.int_(self.sorted_mean_scores[:,0])][::-1])\n",
    "\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(out_path + '_sorted_scores_per_timestep_all_channels')\n",
    "    \n",
    "    def plot_power_heatmap(self, out_path:str):\n",
    "        \"\"\"Plot heatmaps of high/low frequency powers, difference in high/low frequency powers, and LDA coefficients.\"\"\"\n",
    "        num_freqs = self._num_freqs\n",
    "        time_resolution = self._time_resolution\n",
    "        timesteps_rescaled = self._timesteps_rescaled\n",
    "\n",
    "        low_bet_powers = self.low_bet_avg_powers\n",
    "        high_bet_powers = self.high_bet_avg_powers\n",
    "        diff_bet_powers = self.diff_avg_powers\n",
    "        lda_coef = self.lda_coefs\n",
    "\n",
    "        if timesteps_rescaled >= 50:\n",
    "            num_xticks = int(timesteps_rescaled/2)\n",
    "        else:\n",
    "            num_xticks = timesteps_rescaled\n",
    "\n",
    "        if num_freqs == 5: \n",
    "            # sets the y-ticks when using EEG frequency bands\n",
    "            yticks = np.arange(diff_bet_powers.shape[1], step=5)\n",
    "        else:\n",
    "            # sets the y-ticks when using all frequencies in data\n",
    "            yticks = np.arange(diff_bet_powers.shape[1], step=63)\n",
    "            # yticklabels = [round(i,1) for i in np.logspace(np.log2(2),np.log2(150), num = len(yticks),base=2, dtype=np.float16)]\n",
    "\n",
    "        xticks = np.linspace(0, timesteps_rescaled - 1, num=num_xticks, dtype=np.int_)\n",
    "        xticklabels = np.around(np.linspace(0, timesteps_rescaled - 1, num=num_xticks, dtype=np.int_)/(20/time_resolution) - self.__event_delay, decimals=2)\n",
    "\n",
    "        fig, axs = plt.subplots(2, 2,figsize=(24, 20))\n",
    "        \n",
    "        # Plot power per frequency as a function of time, power averaged across all respective trials (high or low bet trials) \n",
    "        sns.heatmap(high_bet_powers.T, ax=axs[0][0], vmin=-.4, vmax=.4, cbar_kws={\"label\": \"Z-Scored Frequency Power\"}, cmap='PRGn')\n",
    "        axs[0][0].set_title('High Bet Z-Scored Frequency Power')\n",
    "\n",
    "        sns.heatmap(low_bet_powers.T, ax=axs[0][1], vmin=-.4, vmax=.4, cbar_kws={\"label\": \"Z-Scored Frequency Power\"}, cmap='PRGn')\n",
    "        axs[0][1].set_title('Low Bet Z-Scored Frequency Power')\n",
    "\n",
    "        # Plots the difference in power frequency for high and low bet trials\n",
    "        sns.heatmap(diff_bet_powers.T, ax=axs[1][0], vmin=-.4, vmax=.4, cbar_kws={\"label\": \"Z-Scored Frequency Power\", \"pad\": 0.1}, cmap='PRGn')\n",
    "        axs[1][0].set_title('Difference in Z-Scored Frequency Power (High - Low Bet)')\n",
    "        ax = axs[1][0].twinx()\n",
    "        sns.lineplot(x=np.arange(0,timesteps_rescaled), y=self.mean_scores.flatten(), color='blue', ax=ax) # Make the overlayed metric an optional variable user can select\n",
    "        ax.set_ylabel('Mean LDA Score')\n",
    "\n",
    "        # Plots the LDA coefficients for each frequency band over time\n",
    "        sns.heatmap(lda_coef.T, ax=axs[1][1], vmin=-1, vmax=1, cbar_kws={\"label\": \"Z-Scored Frequency Power\"}, cmap='PRGn')\n",
    "        axs[1][1].set_title('LDA coefficient values (on channels and frequencies)')\n",
    "\n",
    "        for axs_ in axs:\n",
    "            for ax in axs_:\n",
    "                ax.set_xticks(xticks, labels = xticklabels, rotation = 90)\n",
    "                ax.set_yticks(yticks)\n",
    "                ax.set(xlabel=\"Time (seconds)\", ylabel=\"Frequency Bands * Channels\")\n",
    "                ax.axes.invert_yaxis()\n",
    "                ax.axvline(self.sorted_mean_scores[0,0], color = 'red', alpha=0.5)\n",
    "                ax.axvline(12, color = 'blue', alpha=1, ls = '--')\n",
    "\n",
    "                for label in ax.xaxis.get_ticklabels()[1::2]:\n",
    "                    label.set_visible(False)\n",
    "\n",
    "        plt.savefig(out_path + '_heatmap_per_timestep_all_channels')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_contributing_channels(self, alpha):\n",
    "        \"\"\"Visualize the heatmap of the LDA coefficients for top channels with mean LDA coefficient absolute values greater than alpha.\"\"\"\n",
    "        num_freqs = self._num_freqs\n",
    "        time_resolution = self._time_resolution\n",
    "        timesteps_rescaled = self._timesteps_rescaled\n",
    "\n",
    "        if timesteps_rescaled >= 50:\n",
    "            num_xticks = int(timesteps_rescaled/2)\n",
    "        else:\n",
    "            num_xticks = timesteps_rescaled\n",
    "        \n",
    "        yticks = np.arange(num_freqs)\n",
    "        yticklabels = [\"Delta\", \"Theta\", \"Alpha\", \"Beta\", \"Gamma\"]\n",
    "\n",
    "        xticks = np.linspace(0, timesteps_rescaled - 1, num=num_xticks, dtype=np.int_)\n",
    "        xticklabels = np.around(np.linspace(0, timesteps_rescaled - 1, num=num_xticks, dtype=np.int_)/(20/time_resolution) - self.__event_delay, decimals=2)\n",
    "\n",
    "\n",
    "        avg_coefs = np.abs(self.lda_coefs).mean(0)\n",
    "        idxs = []\n",
    "\n",
    "        for i in avg_coefs.argsort()[::-1]:\n",
    "            if avg_coefs[i] > alpha:\n",
    "                idxs.append(i)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        channels = [int(i/self._num_freqs) for i in idxs]\n",
    "\n",
    "        fig, axs = plt.subplots(len(idxs), 1, figsize=(12,7*len(idxs)))\n",
    "\n",
    "        for i, ch in enumerate(channels):\n",
    "            ch_idx = ch*self._num_freqs\n",
    "            sns.heatmap(self.lda_coefs.T[ch_idx:ch_idx+self._num_freqs], ax=axs[i], cmap='PRGn')\n",
    "\n",
    "            axs[i].set_xticks(xticks, labels = xticklabels, rotation = 90)\n",
    "            axs[i].set_xlabel('Time (seconds)')\n",
    "            axs[i].set_yticklabels(yticklabels, rotation = 0)\n",
    "            axs[i].set_ylabel('Frequency Band')\n",
    "            axs[i].axes.invert_yaxis()\n",
    "            axs[i].axvline(self.sorted_mean_scores[0,0], color = 'red', alpha=0.5)\n",
    "            axs[i].axvline(12, color = 'blue', alpha=1, ls = '--')\n",
    "            axs[i].set_title('Electrode %s in the %s \\n LDA Coefficients' %(self._elec_names[ch], self._elec_areas[ch]))\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "    def train_on_all_channels(self, data, y, time_resolution, filter_channels:bool = True, custom_channels = None):\n",
    "        \"\"\"Train an LDA model on all the channels for each timestep\"\"\"\n",
    "        super().set_attributes(time_resolution=time_resolution)\n",
    "\n",
    "        for time in range(self._timesteps_rescaled):\n",
    "            if filter_channels:\n",
    "                filtered_elec_areas_idxs, _, __ = super().filter_channels()\n",
    "                X = super().create_X(data, filtered_elec_areas_idxs, time*self._time_resolution)\n",
    "            elif not custom_channels == None:\n",
    "                X = super().create_X(data, custom_channels, time*self._time_resolution)\n",
    "            else:\n",
    "                X = super().create_X(data, np.arange(0,self._num_channels), time*self._time_resolution)\n",
    "\n",
    "            X_reshaped = X.reshape(self._num_trials,-1)\n",
    "            super().train(X_reshaped,y)\n",
    "        \n",
    "        super()._reshape_attributes((self._timesteps_rescaled,-1))\n",
    "        self._sort_scores()\n",
    "        self._convert_timesteps_to_time(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf6bb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllTimesteps(LDA):\n",
    "    def train_on_all_timesteps(self, data, y):\n",
    "        \"\"\"Train an LDA model on all the timesteps for each channel\"\"\"\n",
    "        for channel in range(self._num_channels):\n",
    "            X = super().create_X(data, channel, np.arange(self._num_timesteps))\n",
    "            X_reshaped = X.reshape(self._num_trials, -1)\n",
    "            super().train(X_reshaped, y)\n",
    "        \n",
    "        super()._reshape_attributes((self._num_channels,-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1545820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffledLDA(PerChannelTimestep):\n",
    "    def __init__(self, data, setup_data):\n",
    "        super().__init__(data, setup_data)\n",
    "        self._reset_metrics()\n",
    "        \n",
    "    def _get_hand(self, setup_data):\n",
    "        \"\"\"Get the subject's card value at a particular trial\"\"\"\n",
    "        bets = setup_data['filters']['bets']\n",
    "        good_trials = np.where(np.isnan(bets) == False)[0] # extract indices of trials without the 'nan'\n",
    "\n",
    "        self.__sub_hand = setup_data['filters']['card1'][good_trials] # get the subject's card hand for the good trials\n",
    "    \n",
    "    def _shuffle_y(self, y, setup_data):\n",
    "        \"\"\"\n",
    "        Randomly shuffle the elements of y to be in different locations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : arr, required\n",
    "            The labels that the LDA model is to be trained on\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_shuffled : arr\n",
    "            An array with the elements of y randomly shuffled to be in different locations.\n",
    "        \"\"\"\n",
    "        \n",
    "        np.random.seed()\n",
    "\n",
    "        print('Shuffling!')\n",
    "\n",
    "        # Get the locations for each particular card value\n",
    "        self._get_hand(setup_data)\n",
    "        card_value_indices = []\n",
    "        for i in [2,4,6,8,10]:\n",
    "            card_value_indices.append(np.where(self.__sub_hand == i)[0])\n",
    "\n",
    "        y_shuffled = np.zeros(y.shape)\n",
    "\n",
    "        # Ensure that the number of high bets in the shuffled y labels is consistent with the card value\n",
    "        for indices in card_value_indices:\n",
    "            temp = indices\n",
    "            num_high_bets = y[indices].sum() + round(np.random.uniform(-1,1)*y[indices].sum()*0.2) # Get the number of high bets for a particular card value and add some randomness to it\n",
    "            for _ in range(num_high_bets):\n",
    "                if np.any(temp):\n",
    "                    # Pick a random location from all possible locations of that particular card value and set it to 1 (ie high bet)\n",
    "                    rand = np.random.choice(temp)\n",
    "                    y_shuffled[rand] = 1\n",
    "                    rand_index = np.where(temp == rand)[0]\n",
    "                    temp = np.delete(temp,rand_index) # Remove that location from being able to be chosen again\n",
    "            y_shuffled[temp] = 0 # set all other locations for that particular card value to 0 (ie low bet)\n",
    "\n",
    "        return y_shuffled\n",
    "    \n",
    "    def train_per_channel_and_timestep(self, data, y, setup_data, time_resolution):\n",
    "        \"\"\"Use shuffled y labels to train LDA model on each channel and timestep\"\"\"\n",
    "        y_shuffled = self._shuffle_y(y, setup_data)\n",
    "        super().train_per_channel_and_timestep(data, y_shuffled, setup_data=setup_data, time_resolution=time_resolution)\n",
    "\n",
    "    def compute_t_stat_clusters(self, ref_estimator, threshold):\n",
    "        \"\"\"Computes the sum of the t-statistics for a cluster specified by a reference LDA estimator\"\"\"\n",
    "        t_stat_sums = []\n",
    "\n",
    "        for channel in range(self._num_channels):\n",
    "            temp_t_stat_sums = []\n",
    "            for arr in ref_estimator.create_cluster_idxs(threshold)[channel]:\n",
    "                temp_t_stat_sums.append(self.t_stats[channel][arr].sum())\n",
    "            \n",
    "            t_stat_sums.append(temp_t_stat_sums)\n",
    "        \n",
    "        return t_stat_sums"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de898049",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Using the power per wavelet scale for a particular channel and timepoint as a feature. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb630541",
   "metadata": {},
   "source": [
    "## Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86d80f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shuffled_t_stats(data, y, setup_data,  time_resolution, threshold, ref_estimator):\n",
    "    shuffled_lda = ShuffledLDA(setup_data)\n",
    "    shuffled_lda.train_per_channel_and_timestep(data, y, setup_data = setup_data, time_resolution=time_resolution)\n",
    "    return shuffled_lda.compute_t_stat_clusters(ref_estimator, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5388406b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n",
      "Shuffling!\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "n_processes = 20\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with Pool(n_processes) as p:\n",
    "        results = p.starmap(get_shuffled_t_stats, [(f_band_data, y, setup_data, 5, 12, lda)] * 100)\n",
    "        p.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56803640",
   "metadata": {},
   "source": [
    "# Visualization of Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd387a8",
   "metadata": {},
   "source": [
    "# Extraneous Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
